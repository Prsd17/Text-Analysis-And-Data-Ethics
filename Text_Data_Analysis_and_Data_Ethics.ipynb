{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCAPWR2s3nzA"
      },
      "source": [
        "# Text Data Analysis and Data Ethics\n",
        "\n",
        "In this question you will write Python code for processing, analyzing and understanding the social network **Reddit** (www.reddit.com). Reddit is a platform that allows users to upload posts and comment on them, and is divided in _subreddits_, often covering specific themes or areas of interest (for example, [world news](https://www.reddit.com/r/worldnews/), [ukpolitics](https://www.reddit.com/r/ukpolitics/) or [nintendo](https://www.reddit.com/r/nintendo)). You are provided with a subset of Reddit with posts from Covid-related subreddits (e.g., _CoronavirusUK_ or _NoNewNormal_), as well as randomly selected subreddits (e.g., _donaldtrump_ or _razer_).\n",
        "\n",
        "The `csv` dataset you are provided contains one row per post, and has information about three entities: **posts**, **users** and **subreddits**. The column names are self-explanatory: columns starting with the prefix `user_` describe users, those starting with the prefix `subr_` describe subreddits, the `subreddit` column is the subreddit name, and the rest of the columns are post attributes (`author`, `posted_at`, `title` and post text - the `selftext` column-, number of comments - `num_comments`, `score`, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Pm74v1u4d6G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11cccf37-8d5a-4e0b-9d4b-a5d3d9ed6c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "# suggested imports\n",
        "import pandas as pd\n",
        "from nltk.tag import pos_tag\n",
        "import re\n",
        "from collections import defaultdict,Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "tqdm.pandas()\n",
        "from ast import literal_eval\n",
        "# nltk imports, note that these outputs may be different if you are using colab or local jupyter notebooks\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WfNsDQ253nzJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea611508-b287-4bc5-840f-f6a747bc9103"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\n"
          ]
        }
      ],
      "source": [
        "from urllib import request\n",
        "import pandas as pd\n",
        "module_url = f\"https://raw.githubusercontent.com/luisespinosaanke/cmt309-portfolio/master/data_portfolio_21.csv\"\n",
        "module_name = module_url.split('/')[-1]\n",
        "print(f'Fetching {module_url}')\n",
        "#with open(\"file_1.txt\") as f1, open(\"file_2.txt\") as f2\n",
        "with request.urlopen(module_url) as f, open(module_name,'w') as outf:\n",
        "  a = f.read()\n",
        "  outf.write(a.decode('utf-8'))\n",
        "\n",
        "\n",
        "df = pd.read_csv('data_portfolio_21.csv')\n",
        "# this fills empty cells with empty strings\n",
        "df = df.fillna('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNfbxg2X3nzK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "outputId": "d1443a0b-af47-4987-e873-66ff9f17de22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       author            posted_at  num_comments  score selftext  \\\n",
              "0  -Howitzer-  2020-08-17 20:26:04            19      1            \n",
              "1  -Howitzer-  2020-07-06 17:01:48             1      3            \n",
              "2  -Howitzer-  2020-09-09 02:29:02             3      1            \n",
              "3  -Howitzer-  2020-06-23 23:02:39             2      1            \n",
              "4  -Howitzer-  2020-08-07 04:13:53            32    622            \n",
              "\n",
              "  subr_created_at              subr_description  \\\n",
              "0      2009-04-29  Subreddit about Donald Trump   \n",
              "1      2009-04-29  Subreddit about Donald Trump   \n",
              "2      2009-04-29  Subreddit about Donald Trump   \n",
              "3      2009-04-29  Subreddit about Donald Trump   \n",
              "4      2009-04-29  Subreddit about Donald Trump   \n",
              "\n",
              "                                       subr_faved_by  subr_numb_members  \\\n",
              "0  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "1  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "2  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "3  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "4  ['vergil_never_cry', 'Jelegend', 'pianoyeah', ...              30053   \n",
              "\n",
              "   subr_numb_posts    subreddit  \\\n",
              "0           796986  donaldtrump   \n",
              "1           796986  donaldtrump   \n",
              "2           796986  donaldtrump   \n",
              "3           796986  donaldtrump   \n",
              "4           796986  donaldtrump   \n",
              "\n",
              "                                               title  total_awards_received  \\\n",
              "0  BREAKING: Trump to begin hiding in mailboxes t...                      0   \n",
              "1                                Joe Biden's America                      0   \n",
              "2  4 more years and we can erase his legacy for g...                      0   \n",
              "3  Revelation 9:6 [Transhumanism: The New Religio...                      0   \n",
              "4                                     LOOK HERE, FAT                      0   \n",
              "\n",
              "   upvote_ratio  user_num_posts user_registered_at  user_upvote_ratio  \n",
              "0          1.00            4661         2012-11-09          -0.658599  \n",
              "1          0.67            4661         2012-11-09          -0.658599  \n",
              "2          1.00            4661         2012-11-09          -0.658599  \n",
              "3          1.00            4661         2012-11-09          -0.658599  \n",
              "4          0.88            4661         2012-11-09          -0.658599  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4ace7d98-da54-4a1e-afc4-6bddf8951b92\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>posted_at</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>score</th>\n",
              "      <th>selftext</th>\n",
              "      <th>subr_created_at</th>\n",
              "      <th>subr_description</th>\n",
              "      <th>subr_faved_by</th>\n",
              "      <th>subr_numb_members</th>\n",
              "      <th>subr_numb_posts</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>title</th>\n",
              "      <th>total_awards_received</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>user_num_posts</th>\n",
              "      <th>user_registered_at</th>\n",
              "      <th>user_upvote_ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-17 20:26:04</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>BREAKING: Trump to begin hiding in mailboxes t...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-07-06 17:01:48</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Joe Biden's America</td>\n",
              "      <td>0</td>\n",
              "      <td>0.67</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-09-09 02:29:02</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>4 more years and we can erase his legacy for g...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-06-23 23:02:39</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>Revelation 9:6 [Transhumanism: The New Religio...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-Howitzer-</td>\n",
              "      <td>2020-08-07 04:13:53</td>\n",
              "      <td>32</td>\n",
              "      <td>622</td>\n",
              "      <td></td>\n",
              "      <td>2009-04-29</td>\n",
              "      <td>Subreddit about Donald Trump</td>\n",
              "      <td>['vergil_never_cry', 'Jelegend', 'pianoyeah', ...</td>\n",
              "      <td>30053</td>\n",
              "      <td>796986</td>\n",
              "      <td>donaldtrump</td>\n",
              "      <td>LOOK HERE, FAT</td>\n",
              "      <td>0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4661</td>\n",
              "      <td>2012-11-09</td>\n",
              "      <td>-0.658599</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ace7d98-da54-4a1e-afc4-6bddf8951b92')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4ace7d98-da54-4a1e-afc4-6bddf8951b92 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4ace7d98-da54-4a1e-afc4-6bddf8951b92');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyQyR27z48nr"
      },
      "source": [
        "## P1.1 - Text data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLUtCUL853Ln"
      },
      "source": [
        "### P1.1.1 - Faved by as lists \n",
        "\n",
        "The column `subr_faved_by` contains an array of values (names of redditors who added the subreddit to which the current post was submitted), but unfortunately they are in text format, and you would not be able to process them properly without converting them to a suitable python type. You must convert these string values to Python lists, going from\n",
        "\n",
        "```python\n",
        "'[\"user1\", \"user2\" ... ]'\n",
        "```\n",
        "\n",
        "to\n",
        "\n",
        "```python\n",
        "[\"user1\", \"user2\" ... ]\n",
        "```\n",
        "\n",
        "**What to implement:** Implement a function `transform_faves(df)` which takes as input the original dataframe and returns the same dataframe, but with one additional column called `subr_faved_by_as_list`, where you have the same information as in `subr_faved_by`, but as a python list instead of a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJLEddGE56qw"
      },
      "outputs": [],
      "source": [
        "def transform_faves(df):\n",
        "  \"\"\"In this fuction all string values from subr faved by list converted into\n",
        "  list form using literal_eval library...\"\"\"\n",
        "  \n",
        "  #creating new column with updated list values.\n",
        "  df['subr_faved_by_as_list'] = df['subr_faved_by'].apply(eval)\n",
        "  \n",
        "  return df\n",
        "\n",
        "df = transform_faves(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['subr_faved_by_as_list'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOQsDc7QMoM-",
        "outputId": "797622a4-86e1-4b02-ae48-8811614d8f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vergil_never_cry',\n",
              " 'Jelegend',\n",
              " 'pianoyeah',\n",
              " 'salomon_fish',\n",
              " 'ShareRedMedia',\n",
              " 'LateData',\n",
              " 'chrisalves11',\n",
              " 's4nskrit',\n",
              " 'aqua[***]s',\n",
              " 'Ardoriccardo00',\n",
              " 'RaoulDuke209',\n",
              " 'ry_ta506',\n",
              " 'n1ght_w1ng08',\n",
              " 'ElectronicFudge5',\n",
              " 'Roka-chan',\n",
              " 'ujahir18',\n",
              " 'sickestinvertebrate',\n",
              " 'scorpionman',\n",
              " 'bitemedue',\n",
              " 'ostonox',\n",
              " 'sixoneniner89',\n",
              " 'Caiyul',\n",
              " 'jwtv_',\n",
              " 'KarmaFury',\n",
              " 'jigsawmap',\n",
              " 'Monky11',\n",
              " 'rabbits[***]ogue',\n",
              " 'casualphilosopher1',\n",
              " 'zauberpilz_',\n",
              " 'Ghostface908',\n",
              " 'justanotherlidian',\n",
              " 'MissMemmyMei',\n",
              " 'atomolayanatomay',\n",
              " 'HugeDetective0',\n",
              " 'jaimelancaster',\n",
              " 'Salmoninstomach',\n",
              " 'ReactionAndy',\n",
              " 'icedpickles',\n",
              " 'adambc91',\n",
              " 'BrackInMyBrack_',\n",
              " 'TheRealDallasCowboys',\n",
              " 'DarAR92',\n",
              " 'samarai4444',\n",
              " 'numanh[***]an9035',\n",
              " 'Evoxrus_XV',\n",
              " 'anbeck',\n",
              " 'klondipedia',\n",
              " 'joyousjoyness',\n",
              " 'Dollar99Man',\n",
              " 'Mental-Day',\n",
              " 'penirosmind1',\n",
              " 'stuuked',\n",
              " 'platxerath',\n",
              " 'polskeetskeet',\n",
              " 'CheetahSperm18',\n",
              " 'biobio1337',\n",
              " 'Chobits_',\n",
              " 'thicc_Nword',\n",
              " 'yofred',\n",
              " '___TheKid___',\n",
              " 'winterdates',\n",
              " 'Dystopiannie',\n",
              " 'danceswithtools',\n",
              " 'mahoney4321']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZ3u5aS3rrm"
      },
      "source": [
        "### P1.1.2 - Merge titles and text bodies\n",
        "\n",
        "All Reddit posts need to have a title, but a text body is optional. However, we want to be able to access all free text information for each post without having to look at two columns every time.\n",
        "\n",
        "**What to implement**: A function `concat(df)` that will take as input the original dataframe and will return it with an additional column called `full_text`, which will concatenate `title` and `selftext` columns, but with the following restrictions:\n",
        "\n",
        "- 1) Wrap the title between `<title>` and `</title>` tags.\n",
        "- 2) Add a new line (`\\n`) between title and selftext, but only in cases where you have both values (see instruction 4).\n",
        "- 3) Wrap the selftext between `<selftext>` and `</selftext>`.\n",
        "- 4) You **must not** include the tags in points (1) or (3) if the values for these columns is missing. We will consider a missing value either an empty value (empty string) or a string of only one character (e.g., an emoji). Also, the value of a `full_text` column must not end in the new line character."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsmY-JB39N2m"
      },
      "outputs": [],
      "source": [
        "def concat(df):\n",
        "  \"\"\"This function returns all title and selftext values inside of wrap tags...\"\"\"\n",
        "  \n",
        "  df['full_text'] = ''\n",
        "  for index, row in df.iterrows():\n",
        "    #creating empty strings for title and selftext.\n",
        "    title = '' \n",
        "    selftext = ''   \n",
        "    \n",
        "    #if title is not empty then adding in between tags.\n",
        "    if len(row['title'].strip()) > 1:\n",
        "        title = '<title>' + row['title'] + '</title>'\n",
        "   \n",
        "    #if selftext is not empty then adding in between tags.\n",
        "    if len(row['selftext'].strip()) > 1:\n",
        "        selftext = '<selftext>' + row['selftext'] + '</selftext>'\n",
        "        title += '\\n'  \n",
        "    \n",
        "    #updating the values in the full text column.   \n",
        "    df.loc[index, 'full_text'] = title + selftext\n",
        "                \n",
        "  return df\n",
        "\n",
        "df = concat(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['full_text'][0]"
      ],
      "metadata": {
        "id": "iSRfTIStodr9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f0ea810d-a282-460d-c2d1-74ba093e3d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<title>BREAKING: Trump to begin hiding in mailboxes to destroy mail-in ballots.</title>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['full_text'][47]"
      ],
      "metadata": {
        "id": "zJwR7oSIiEOD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "a5a2fb47-467d-4f44-c62c-0041c7b209b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<title>The vaccine is a cover up for the upcoming huge number of deaths of people just getting really sick and dying in the middle of a street like in the videos we saw in wuhan. those deaths will be blamed on vaccines not on the bioweapon that covid is, read the post text for my arguments</title>\\n<selftext>this theory of mine is based on the [***]umption that covid is a bioweapon that got out of control as an accident or more likely as an attack from an unknown enemy.\\n\\nif we had someone or some group releasing this bioweapon on purpose then we can understand the lockdowns as a reaction of the state in trying to contain this group/person to a certain area and maybe catch him if he/they try to get out or move to other areas.\\n\\nfirst by locking down, wuhan, then china, then some other parts of asia, once it got to europe, immediately locking up italy.\\n\\nthat plan failed\\n\\nmeanwhile in wuhan we had a huge number of deaths scaring people out of their minds and making them go over the fear of the communist regime and try to get out. a sing of this is the last real news we've got from wuhan of the videos of the battle over the bridge where hundred of thousands of people, common citizens and police forces from wuhan fought against the chinese armed forces to try to get out of the area using the bridge.\\n\\nwe don't know how it ended there, we can speculate that most likely they were forced back.\\n\\ndid the worse p[***]ed in in china? how many deaths they had? did the supposedly trains of people going to reeducation camps are actually refugees from areas under attack?\\n\\nanyway, back to present day, we still have the threat and we still the need to avoid widespread m[***] panic that if it starts it means the end of our society, and a huge number of deaths because of disruption in the logistic support, power outage, civil unrest. and on an on\\n\\nso how to explain people starting dying around you with o real explanation?\\n\\nso because off:\\n\\nthe fact that the diverse governments all over the world that usually we can't call friends, like for example russia and the united states, basically engaged in a proxy war in Syria for years, all act on covid on the same line and don't take advantage of their enemy momentarily difficulty\\n\\nthen we top the fact that all corporations, financial powers, markets while loosing a lot of money they don't react against covid\\n\\nand finally not even the criminals like various mafias around the world reacted against this and boy did they lose a lot of money from various activities they do like gambling or drugs and the like.\\n\\nall these big players, they don't react. they know that this is the minor evil, they know what is at stake here\\n\\nonly people that react are small business owners and normal people that obviously are not informed about the real situation.\\n\\n**here comes the vaccine plan**\\n\\nyou create a vaccine, you make sure that everyone and their mothers see that the first person you vaccinated on live tv ends up fainting, then gets to the emergency room then you make sure that it is absolutely obvious that she is dead.\\n\\nthen the mainstream media keep showing people dying after taking the vaccine, having a wide range of health reactions, strokes, paralyses, shocks, respiratory issues and on and on.\\n\\nbecause it would be very easy for mainstream media to hide any adverse vaccine reactions as they did it for the years for the children ending with autism disorders after vaccinations you have to ask yourself why they make sure you know that the covid vaccine have these particular sets of reactions that match perfectly with what we have seen in the videos from wuhan one year ago.\\n\\nif is true that this bioweaon is a slow burn that eats and eats the body till one can't take it anymore then we are near the time when we will start experiencing what we have seen in wuhan last year.\\n\\nfinally we have the current reality where US just wake up in a war like emergency where the military are on the streets supposedly because there are some rednecks that try to destabilize the democracy.\\n\\nyou can see that they don't care about some self proclaimed militia that curiously they can't show any footage about them, and if they would know who they are again curiously why not go there and arrest them instead of camping like a camper waiting for these rednecks to travel over state roads, organize and what not.</selftext>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADWvbAIe4TVd"
      },
      "source": [
        "### P1.1.3 - Enrich posts \n",
        "\n",
        "We would like to augment our text data with linguistic information. To this end, we will _tokenize_, apply _part-of-speech tagging_, and then we will _lower case_ all the posts.\n",
        "\n",
        "**What to implement**: A function `enrich_posts(df)` that will take as input the original dataframe and will return it with **two** additional columns: `enriched_title` and `enriched_selftext`. These columns will contain tokenized, pos-tagged and lower cased versions of the original text. **You must implement them in this order**, because the pos tagger uses casing information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nDnaSwI46T_"
      },
      "outputs": [],
      "source": [
        "def enrich_posts(df):\n",
        "  \"\"\"This function returns all words with part of speech tagging for selftext \n",
        "  and title column. also words converted into lowercase...\"\"\"\n",
        "  \n",
        "  #converting all title and selftext values in to list form. \n",
        "  title = df['title'].values.tolist()\n",
        "  self_text = df['selftext'].values.tolist()\n",
        "  \n",
        "  #empty list for updating values. \n",
        "  titletext = []\n",
        "  selftext = []\n",
        "  \n",
        "  #for all words in title convert into part of speech using nltk postag and lowering it.\n",
        "  for word in title:\n",
        "    titletext.append(nltk.pos_tag(word_tokenize(word.lower())))\n",
        "    \n",
        "    #new column for title tagposts.\n",
        "    df['enriched_title'] = pd.Series(titletext)\n",
        " \n",
        "  #for all words in selftext convert into part of speech using nltk postag and lowering it.\n",
        "  for word in self_text:\n",
        "    selftext.append(nltk.pos_tag(word_tokenize(word.lower())))\n",
        "      \n",
        "    #new column for selftext tagposts.  \n",
        "    df['enriched_selftext'] = pd.Series(selftext)\n",
        " \n",
        "  return df\n",
        "\n",
        "df = enrich_posts(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['enriched_title'][0]"
      ],
      "metadata": {
        "id": "TBL_TS9Bpbix",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34226713-3d32-4e9b-9752-a56bb3f90bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('breaking', 'NN'),\n",
              " (':', ':'),\n",
              " ('trump', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('begin', 'VB'),\n",
              " ('hiding', 'VBG'),\n",
              " ('in', 'IN'),\n",
              " ('mailboxes', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('destroy', 'VB'),\n",
              " ('mail-in', 'JJ'),\n",
              " ('ballots', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['enriched_selftext'][47]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjLDC0NdMbKO",
        "outputId": "0aedd487-a720-4a69-9ffb-51f8b0abd73d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('this', 'DT'),\n",
              " ('theory', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('mine', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('based', 'VBN'),\n",
              " ('on', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('[', 'NNP'),\n",
              " ('***', 'NNP'),\n",
              " (']', 'NNP'),\n",
              " ('umption', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('covid', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('bioweapon', 'NN'),\n",
              " ('that', 'WDT'),\n",
              " ('got', 'VBD'),\n",
              " ('out', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('control', 'NN'),\n",
              " ('as', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('accident', 'NN'),\n",
              " ('or', 'CC'),\n",
              " ('more', 'RBR'),\n",
              " ('likely', 'JJ'),\n",
              " ('as', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('attack', 'NN'),\n",
              " ('from', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('unknown', 'JJ'),\n",
              " ('enemy', 'NN'),\n",
              " ('.', '.'),\n",
              " ('if', 'IN'),\n",
              " ('we', 'PRP'),\n",
              " ('had', 'VBD'),\n",
              " ('someone', 'NN'),\n",
              " ('or', 'CC'),\n",
              " ('some', 'DT'),\n",
              " ('group', 'NN'),\n",
              " ('releasing', 'VBG'),\n",
              " ('this', 'DT'),\n",
              " ('bioweapon', 'NN'),\n",
              " ('on', 'IN'),\n",
              " ('purpose', 'JJ'),\n",
              " ('then', 'RB'),\n",
              " ('we', 'PRP'),\n",
              " ('can', 'MD'),\n",
              " ('understand', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('lockdowns', 'NN'),\n",
              " ('as', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('reaction', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('state', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('trying', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('contain', 'VB'),\n",
              " ('this', 'DT'),\n",
              " ('group/person', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('a', 'DT'),\n",
              " ('certain', 'JJ'),\n",
              " ('area', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('maybe', 'RB'),\n",
              " ('catch', 'VB'),\n",
              " ('him', 'PRP'),\n",
              " ('if', 'IN'),\n",
              " ('he/they', 'VBN'),\n",
              " ('try', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('out', 'RP'),\n",
              " ('or', 'CC'),\n",
              " ('move', 'VB'),\n",
              " ('to', 'TO'),\n",
              " ('other', 'JJ'),\n",
              " ('areas', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('first', 'JJ'),\n",
              " ('by', 'IN'),\n",
              " ('locking', 'VBG'),\n",
              " ('down', 'RP'),\n",
              " (',', ','),\n",
              " ('wuhan', 'NN'),\n",
              " (',', ','),\n",
              " ('then', 'RB'),\n",
              " ('china', 'NN'),\n",
              " (',', ','),\n",
              " ('then', 'RB'),\n",
              " ('some', 'DT'),\n",
              " ('other', 'JJ'),\n",
              " ('parts', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('asia', 'NN'),\n",
              " (',', ','),\n",
              " ('once', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('got', 'VBD'),\n",
              " ('to', 'TO'),\n",
              " ('europe', 'VB'),\n",
              " (',', ','),\n",
              " ('immediately', 'RB'),\n",
              " ('locking', 'VBG'),\n",
              " ('up', 'RP'),\n",
              " ('italy', 'NN'),\n",
              " ('.', '.'),\n",
              " ('that', 'DT'),\n",
              " ('plan', 'NN'),\n",
              " ('failed', 'VBD'),\n",
              " ('meanwhile', 'RB'),\n",
              " ('in', 'IN'),\n",
              " ('wuhan', 'NN'),\n",
              " ('we', 'PRP'),\n",
              " ('had', 'VBD'),\n",
              " ('a', 'DT'),\n",
              " ('huge', 'JJ'),\n",
              " ('number', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('deaths', 'NNS'),\n",
              " ('scaring', 'VBG'),\n",
              " ('people', 'NNS'),\n",
              " ('out', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('their', 'PRP$'),\n",
              " ('minds', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('making', 'VBG'),\n",
              " ('them', 'PRP'),\n",
              " ('go', 'VB'),\n",
              " ('over', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('fear', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('communist', 'NN'),\n",
              " ('regime', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('try', 'VB'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('out', 'RP'),\n",
              " ('.', '.'),\n",
              " ('a', 'DT'),\n",
              " ('sing', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('this', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('last', 'JJ'),\n",
              " ('real', 'JJ'),\n",
              " ('news', 'NN'),\n",
              " ('we', 'PRP'),\n",
              " (\"'ve\", 'VBP'),\n",
              " ('got', 'VBN'),\n",
              " ('from', 'IN'),\n",
              " ('wuhan', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('videos', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('battle', 'NN'),\n",
              " ('over', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('bridge', 'NN'),\n",
              " ('where', 'WRB'),\n",
              " ('hundred', 'CD'),\n",
              " ('of', 'IN'),\n",
              " ('thousands', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('people', 'NNS'),\n",
              " (',', ','),\n",
              " ('common', 'JJ'),\n",
              " ('citizens', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('police', 'NNS'),\n",
              " ('forces', 'NNS'),\n",
              " ('from', 'IN'),\n",
              " ('wuhan', 'JJ'),\n",
              " ('fought', 'VBN'),\n",
              " ('against', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('chinese', 'JJ'),\n",
              " ('armed', 'VBD'),\n",
              " ('forces', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('try', 'VB'),\n",
              " ('to', 'TO'),\n",
              " ('get', 'VB'),\n",
              " ('out', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('area', 'NN'),\n",
              " ('using', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('bridge', 'NN'),\n",
              " ('.', '.'),\n",
              " ('we', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('know', 'VB'),\n",
              " ('how', 'WRB'),\n",
              " ('it', 'PRP'),\n",
              " ('ended', 'VBD'),\n",
              " ('there', 'RB'),\n",
              " (',', ','),\n",
              " ('we', 'PRP'),\n",
              " ('can', 'MD'),\n",
              " ('speculate', 'VB'),\n",
              " ('that', 'DT'),\n",
              " ('most', 'RBS'),\n",
              " ('likely', 'JJ'),\n",
              " ('they', 'PRP'),\n",
              " ('were', 'VBD'),\n",
              " ('forced', 'VBN'),\n",
              " ('back', 'RB'),\n",
              " ('.', '.'),\n",
              " ('did', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('worse', 'NN'),\n",
              " ('p', 'NN'),\n",
              " ('[', 'NNP'),\n",
              " ('***', 'NNP'),\n",
              " (']', 'NNP'),\n",
              " ('ed', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('in', 'IN'),\n",
              " ('china', 'NN'),\n",
              " ('?', '.'),\n",
              " ('how', 'WRB'),\n",
              " ('many', 'JJ'),\n",
              " ('deaths', 'NNS'),\n",
              " ('they', 'PRP'),\n",
              " ('had', 'VBD'),\n",
              " ('?', '.'),\n",
              " ('did', 'VBD'),\n",
              " ('the', 'DT'),\n",
              " ('supposedly', 'NN'),\n",
              " ('trains', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('people', 'NNS'),\n",
              " ('going', 'VBG'),\n",
              " ('to', 'TO'),\n",
              " ('reeducation', 'VB'),\n",
              " ('camps', 'NNS'),\n",
              " ('are', 'VBP'),\n",
              " ('actually', 'RB'),\n",
              " ('refugees', 'NNS'),\n",
              " ('from', 'IN'),\n",
              " ('areas', 'NNS'),\n",
              " ('under', 'IN'),\n",
              " ('attack', 'NN'),\n",
              " ('?', '.'),\n",
              " ('anyway', 'RB'),\n",
              " (',', ','),\n",
              " ('back', 'RB'),\n",
              " ('to', 'TO'),\n",
              " ('present', 'JJ'),\n",
              " ('day', 'NN'),\n",
              " (',', ','),\n",
              " ('we', 'PRP'),\n",
              " ('still', 'RB'),\n",
              " ('have', 'VBP'),\n",
              " ('the', 'DT'),\n",
              " ('threat', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('we', 'PRP'),\n",
              " ('still', 'RB'),\n",
              " ('the', 'DT'),\n",
              " ('need', 'NN'),\n",
              " ('to', 'TO'),\n",
              " ('avoid', 'VB'),\n",
              " ('widespread', 'JJ'),\n",
              " ('m', 'NN'),\n",
              " ('[', 'NNP'),\n",
              " ('***', 'NNP'),\n",
              " (']', 'NNP'),\n",
              " ('panic', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('if', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('starts', 'VBZ'),\n",
              " ('it', 'PRP'),\n",
              " ('means', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('end', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('our', 'PRP$'),\n",
              " ('society', 'NN'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('a', 'DT'),\n",
              " ('huge', 'JJ'),\n",
              " ('number', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('deaths', 'NNS'),\n",
              " ('because', 'IN'),\n",
              " ('of', 'IN'),\n",
              " ('disruption', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('logistic', 'JJ'),\n",
              " ('support', 'NN'),\n",
              " (',', ','),\n",
              " ('power', 'NN'),\n",
              " ('outage', 'NN'),\n",
              " (',', ','),\n",
              " ('civil', 'JJ'),\n",
              " ('unrest', 'NN'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('on', 'IN'),\n",
              " ('an', 'DT'),\n",
              " ('on', 'IN'),\n",
              " ('so', 'RB'),\n",
              " ('how', 'WRB'),\n",
              " ('to', 'TO'),\n",
              " ('explain', 'VB'),\n",
              " ('people', 'NNS'),\n",
              " ('starting', 'VBG'),\n",
              " ('dying', 'VBG'),\n",
              " ('around', 'IN'),\n",
              " ('you', 'PRP'),\n",
              " ('with', 'IN'),\n",
              " ('o', 'JJ'),\n",
              " ('real', 'JJ'),\n",
              " ('explanation', 'NN'),\n",
              " ('?', '.'),\n",
              " ('so', 'RB'),\n",
              " ('because', 'IN'),\n",
              " ('off', 'NN'),\n",
              " (':', ':'),\n",
              " ('the', 'DT'),\n",
              " ('fact', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('diverse', 'JJ'),\n",
              " ('governments', 'NNS'),\n",
              " ('all', 'DT'),\n",
              " ('over', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('world', 'NN'),\n",
              " ('that', 'WDT'),\n",
              " ('usually', 'RB'),\n",
              " ('we', 'PRP'),\n",
              " ('ca', 'MD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('call', 'VB'),\n",
              " ('friends', 'NNS'),\n",
              " (',', ','),\n",
              " ('like', 'IN'),\n",
              " ('for', 'IN'),\n",
              " ('example', 'NN'),\n",
              " ('russia', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('united', 'JJ'),\n",
              " ('states', 'NNS'),\n",
              " (',', ','),\n",
              " ('basically', 'RB'),\n",
              " ('engaged', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('proxy', 'JJ'),\n",
              " ('war', 'NN'),\n",
              " ('in', 'IN'),\n",
              " ('syria', 'NN'),\n",
              " ('for', 'IN'),\n",
              " ('years', 'NNS'),\n",
              " (',', ','),\n",
              " ('all', 'DT'),\n",
              " ('act', 'NN'),\n",
              " ('on', 'IN'),\n",
              " ('covid', 'NN'),\n",
              " ('on', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('same', 'JJ'),\n",
              " ('line', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('take', 'VB'),\n",
              " ('advantage', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('their', 'PRP$'),\n",
              " ('enemy', 'NN'),\n",
              " ('momentarily', 'RB'),\n",
              " ('difficulty', 'NN'),\n",
              " ('then', 'RB'),\n",
              " ('we', 'PRP'),\n",
              " ('top', 'VBP'),\n",
              " ('the', 'DT'),\n",
              " ('fact', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('all', 'DT'),\n",
              " ('corporations', 'NNS'),\n",
              " (',', ','),\n",
              " ('financial', 'JJ'),\n",
              " ('powers', 'NNS'),\n",
              " (',', ','),\n",
              " ('markets', 'NNS'),\n",
              " ('while', 'IN'),\n",
              " ('loosing', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('lot', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('money', 'NN'),\n",
              " ('they', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('react', 'VB'),\n",
              " ('against', 'IN'),\n",
              " ('covid', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('finally', 'RB'),\n",
              " ('not', 'RB'),\n",
              " ('even', 'RB'),\n",
              " ('the', 'DT'),\n",
              " ('criminals', 'NNS'),\n",
              " ('like', 'IN'),\n",
              " ('various', 'JJ'),\n",
              " ('mafias', 'NNS'),\n",
              " ('around', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('world', 'NN'),\n",
              " ('reacted', 'VBD'),\n",
              " ('against', 'IN'),\n",
              " ('this', 'DT'),\n",
              " ('and', 'CC'),\n",
              " ('boy', 'NN'),\n",
              " ('did', 'VBD'),\n",
              " ('they', 'PRP'),\n",
              " ('lose', 'VB'),\n",
              " ('a', 'DT'),\n",
              " ('lot', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('money', 'NN'),\n",
              " ('from', 'IN'),\n",
              " ('various', 'JJ'),\n",
              " ('activities', 'NNS'),\n",
              " ('they', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " ('like', 'IN'),\n",
              " ('gambling', 'VBG'),\n",
              " ('or', 'CC'),\n",
              " ('drugs', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('the', 'DT'),\n",
              " ('like', 'JJ'),\n",
              " ('.', '.'),\n",
              " ('all', 'PDT'),\n",
              " ('these', 'DT'),\n",
              " ('big', 'JJ'),\n",
              " ('players', 'NNS'),\n",
              " (',', ','),\n",
              " ('they', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('react', 'VB'),\n",
              " ('.', '.'),\n",
              " ('they', 'PRP'),\n",
              " ('know', 'VBP'),\n",
              " ('that', 'IN'),\n",
              " ('this', 'DT'),\n",
              " ('is', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('minor', 'JJ'),\n",
              " ('evil', 'NN'),\n",
              " (',', ','),\n",
              " ('they', 'PRP'),\n",
              " ('know', 'VBP'),\n",
              " ('what', 'WP'),\n",
              " ('is', 'VBZ'),\n",
              " ('at', 'IN'),\n",
              " ('stake', 'NN'),\n",
              " ('here', 'RB'),\n",
              " ('only', 'RB'),\n",
              " ('people', 'NNS'),\n",
              " ('that', 'IN'),\n",
              " ('react', 'NN'),\n",
              " ('are', 'VBP'),\n",
              " ('small', 'JJ'),\n",
              " ('business', 'NN'),\n",
              " ('owners', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('normal', 'JJ'),\n",
              " ('people', 'NNS'),\n",
              " ('that', 'IN'),\n",
              " ('obviously', 'RB'),\n",
              " ('are', 'VBP'),\n",
              " ('not', 'RB'),\n",
              " ('informed', 'VBN'),\n",
              " ('about', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('real', 'JJ'),\n",
              " ('situation', 'NN'),\n",
              " ('.', '.'),\n",
              " ('**here', 'RB'),\n",
              " ('comes', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('vaccine', 'NN'),\n",
              " ('plan**', 'NN'),\n",
              " ('you', 'PRP'),\n",
              " ('create', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('vaccine', 'NN'),\n",
              " (',', ','),\n",
              " ('you', 'PRP'),\n",
              " ('make', 'VBP'),\n",
              " ('sure', 'JJ'),\n",
              " ('that', 'IN'),\n",
              " ('everyone', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('their', 'PRP$'),\n",
              " ('mothers', 'NNS'),\n",
              " ('see', 'VBP'),\n",
              " ('that', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('first', 'JJ'),\n",
              " ('person', 'NN'),\n",
              " ('you', 'PRP'),\n",
              " ('vaccinated', 'VBD'),\n",
              " ('on', 'IN'),\n",
              " ('live', 'JJ'),\n",
              " ('tv', 'NN'),\n",
              " ('ends', 'VBZ'),\n",
              " ('up', 'RP'),\n",
              " ('fainting', 'NN'),\n",
              " (',', ','),\n",
              " ('then', 'RB'),\n",
              " ('gets', 'VBZ'),\n",
              " ('to', 'TO'),\n",
              " ('the', 'DT'),\n",
              " ('emergency', 'NN'),\n",
              " ('room', 'NN'),\n",
              " ('then', 'RB'),\n",
              " ('you', 'PRP'),\n",
              " ('make', 'VBP'),\n",
              " ('sure', 'JJ'),\n",
              " ('that', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('absolutely', 'RB'),\n",
              " ('obvious', 'JJ'),\n",
              " ('that', 'IN'),\n",
              " ('she', 'PRP'),\n",
              " ('is', 'VBZ'),\n",
              " ('dead', 'JJ'),\n",
              " ('.', '.'),\n",
              " ('then', 'RB'),\n",
              " ('the', 'DT'),\n",
              " ('mainstream', 'NN'),\n",
              " ('media', 'NNS'),\n",
              " ('keep', 'VB'),\n",
              " ('showing', 'VBG'),\n",
              " ('people', 'NNS'),\n",
              " ('dying', 'VBG'),\n",
              " ('after', 'IN'),\n",
              " ('taking', 'VBG'),\n",
              " ('the', 'DT'),\n",
              " ('vaccine', 'NN'),\n",
              " (',', ','),\n",
              " ('having', 'VBG'),\n",
              " ('a', 'DT'),\n",
              " ('wide', 'JJ'),\n",
              " ('range', 'NN'),\n",
              " ('of', 'IN'),\n",
              " ('health', 'NN'),\n",
              " ('reactions', 'NNS'),\n",
              " (',', ','),\n",
              " ('strokes', 'NNS'),\n",
              " (',', ','),\n",
              " ('paralyses', 'NNS'),\n",
              " (',', ','),\n",
              " ('shocks', 'NNS'),\n",
              " (',', ','),\n",
              " ('respiratory', 'NN'),\n",
              " ('issues', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('on', 'IN'),\n",
              " ('and', 'CC'),\n",
              " ('on', 'IN'),\n",
              " ('.', '.'),\n",
              " ('because', 'IN'),\n",
              " ('it', 'PRP'),\n",
              " ('would', 'MD'),\n",
              " ('be', 'VB'),\n",
              " ('very', 'RB'),\n",
              " ('easy', 'JJ'),\n",
              " ('for', 'IN'),\n",
              " ('mainstream', 'JJ'),\n",
              " ('media', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('hide', 'VB'),\n",
              " ('any', 'DT'),\n",
              " ('adverse', 'JJ'),\n",
              " ('vaccine', 'NN'),\n",
              " ('reactions', 'NNS'),\n",
              " ('as', 'IN'),\n",
              " ('they', 'PRP'),\n",
              " ('did', 'VBD'),\n",
              " ('it', 'PRP'),\n",
              " ('for', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('years', 'NNS'),\n",
              " ('for', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('children', 'NNS'),\n",
              " ('ending', 'VBG'),\n",
              " ('with', 'IN'),\n",
              " ('autism', 'NN'),\n",
              " ('disorders', 'NNS'),\n",
              " ('after', 'IN'),\n",
              " ('vaccinations', 'NNS'),\n",
              " ('you', 'PRP'),\n",
              " ('have', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('ask', 'VB'),\n",
              " ('yourself', 'PRP'),\n",
              " ('why', 'WRB'),\n",
              " ('they', 'PRP'),\n",
              " ('make', 'VBP'),\n",
              " ('sure', 'JJ'),\n",
              " ('you', 'PRP'),\n",
              " ('know', 'VBP'),\n",
              " ('that', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('covid', 'NN'),\n",
              " ('vaccine', 'NN'),\n",
              " ('have', 'VBP'),\n",
              " ('these', 'DT'),\n",
              " ('particular', 'JJ'),\n",
              " ('sets', 'NNS'),\n",
              " ('of', 'IN'),\n",
              " ('reactions', 'NNS'),\n",
              " ('that', 'WDT'),\n",
              " ('match', 'VBP'),\n",
              " ('perfectly', 'RB'),\n",
              " ('with', 'IN'),\n",
              " ('what', 'WP'),\n",
              " ('we', 'PRP'),\n",
              " ('have', 'VBP'),\n",
              " ('seen', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('videos', 'NN'),\n",
              " ('from', 'IN'),\n",
              " ('wuhan', 'NN'),\n",
              " ('one', 'CD'),\n",
              " ('year', 'NN'),\n",
              " ('ago', 'RB'),\n",
              " ('.', '.'),\n",
              " ('if', 'IN'),\n",
              " ('is', 'VBZ'),\n",
              " ('true', 'JJ'),\n",
              " ('that', 'IN'),\n",
              " ('this', 'DT'),\n",
              " ('bioweaon', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('a', 'DT'),\n",
              " ('slow', 'JJ'),\n",
              " ('burn', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('eats', 'NNS'),\n",
              " ('and', 'CC'),\n",
              " ('eats', 'VBZ'),\n",
              " ('the', 'DT'),\n",
              " ('body', 'NN'),\n",
              " ('till', 'VB'),\n",
              " ('one', 'CD'),\n",
              " ('ca', 'MD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('take', 'VB'),\n",
              " ('it', 'PRP'),\n",
              " ('anymore', 'RB'),\n",
              " ('then', 'RB'),\n",
              " ('we', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('near', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('time', 'NN'),\n",
              " ('when', 'WRB'),\n",
              " ('we', 'PRP'),\n",
              " ('will', 'MD'),\n",
              " ('start', 'VB'),\n",
              " ('experiencing', 'VBG'),\n",
              " ('what', 'WP'),\n",
              " ('we', 'PRP'),\n",
              " ('have', 'VBP'),\n",
              " ('seen', 'VBN'),\n",
              " ('in', 'IN'),\n",
              " ('wuhan', 'NN'),\n",
              " ('last', 'JJ'),\n",
              " ('year', 'NN'),\n",
              " ('.', '.'),\n",
              " ('finally', 'RB'),\n",
              " ('we', 'PRP'),\n",
              " ('have', 'VBP'),\n",
              " ('the', 'DT'),\n",
              " ('current', 'JJ'),\n",
              " ('reality', 'NN'),\n",
              " ('where', 'WRB'),\n",
              " ('us', 'PRP'),\n",
              " ('just', 'RB'),\n",
              " ('wake', 'VB'),\n",
              " ('up', 'RP'),\n",
              " ('in', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('war', 'NN'),\n",
              " ('like', 'IN'),\n",
              " ('emergency', 'NN'),\n",
              " ('where', 'WRB'),\n",
              " ('the', 'DT'),\n",
              " ('military', 'NN'),\n",
              " ('are', 'VBP'),\n",
              " ('on', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('streets', 'NNS'),\n",
              " ('supposedly', 'RB'),\n",
              " ('because', 'IN'),\n",
              " ('there', 'EX'),\n",
              " ('are', 'VBP'),\n",
              " ('some', 'DT'),\n",
              " ('rednecks', 'NNS'),\n",
              " ('that', 'WDT'),\n",
              " ('try', 'VBP'),\n",
              " ('to', 'TO'),\n",
              " ('destabilize', 'VB'),\n",
              " ('the', 'DT'),\n",
              " ('democracy', 'NN'),\n",
              " ('.', '.'),\n",
              " ('you', 'PRP'),\n",
              " ('can', 'MD'),\n",
              " ('see', 'VB'),\n",
              " ('that', 'IN'),\n",
              " ('they', 'PRP'),\n",
              " ('do', 'VBP'),\n",
              " (\"n't\", 'RB'),\n",
              " ('care', 'VB'),\n",
              " ('about', 'IN'),\n",
              " ('some', 'DT'),\n",
              " ('self', 'NN'),\n",
              " ('proclaimed', 'VBD'),\n",
              " ('militia', 'NN'),\n",
              " ('that', 'IN'),\n",
              " ('curiously', 'RB'),\n",
              " ('they', 'PRP'),\n",
              " ('ca', 'MD'),\n",
              " (\"n't\", 'RB'),\n",
              " ('show', 'VB'),\n",
              " ('any', 'DT'),\n",
              " ('footage', 'NN'),\n",
              " ('about', 'IN'),\n",
              " ('them', 'PRP'),\n",
              " (',', ','),\n",
              " ('and', 'CC'),\n",
              " ('if', 'IN'),\n",
              " ('they', 'PRP'),\n",
              " ('would', 'MD'),\n",
              " ('know', 'VB'),\n",
              " ('who', 'WP'),\n",
              " ('they', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('again', 'RB'),\n",
              " ('curiously', 'RB'),\n",
              " ('why', 'WRB'),\n",
              " ('not', 'RB'),\n",
              " ('go', 'VB'),\n",
              " ('there', 'RB'),\n",
              " ('and', 'CC'),\n",
              " ('arrest', 'VB'),\n",
              " ('them', 'PRP'),\n",
              " ('instead', 'RB'),\n",
              " ('of', 'IN'),\n",
              " ('camping', 'VBG'),\n",
              " ('like', 'IN'),\n",
              " ('a', 'DT'),\n",
              " ('camper', 'NN'),\n",
              " ('waiting', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('these', 'DT'),\n",
              " ('rednecks', 'NNS'),\n",
              " ('to', 'TO'),\n",
              " ('travel', 'VB'),\n",
              " ('over', 'IN'),\n",
              " ('state', 'NN'),\n",
              " ('roads', 'NNS'),\n",
              " (',', ','),\n",
              " ('organize', 'VB'),\n",
              " ('and', 'CC'),\n",
              " ('what', 'WP'),\n",
              " ('not', 'RB'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8E010UbQyML"
      },
      "source": [
        "## P1.2 - Answering questions with pandas\n",
        "\n",
        "In this question, your task is to use pandas to answer questions about the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZmG2VIYQ93I"
      },
      "source": [
        "### P1.2.1 - Users with best scores\n",
        "\n",
        "- Find the users with the highest aggregate scores (over all their posts) for the whole dataset. You should restrict your results to only those whose aggregated score is above 10,000 points, in descending order. Your code should generate a dictionary of the form `{author:aggregated_scores ... }`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new df which contans author and score columns.\n",
        "df_new = df[['author','score']]\n",
        "\n",
        "#finding all author whos sum is greater than 10000. \n",
        "agg_score = df_new.groupby(['author']).sum().query('score > 10000').sort_values('score').reset_index()\n",
        "\n",
        "#checking values.\n",
        "agg_score\n",
        "\n",
        "#converting data frame into dictionary.\n",
        "a= dict(zip(agg_score.author, agg_score.score))\n",
        "\n",
        "#the following function gives the reverse dictionary.\n",
        "def descending_order(d):\n",
        "  \"\"\"Reason of Function: google colab unable to reverse dictionary directly...\"\"\"\n",
        "  \n",
        "  #sorting all items in reverse order.\n",
        "  return dict(sorted(d.items(), key= lambda x: x[1],reverse = True))\n",
        "\n",
        "#final output dictionary.\n",
        "print(descending_order(a))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8E9WCaNdkTY",
        "outputId": "10c8b1d0-2d30-4305-a9f6-b23a6380d7a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'DaFunkJunkie': 250375, 'None': 218846, 'SUPERGUESSOUS': 211611, 'jigsawmap': 210824, 'chrisdh79': 143538, 'hildebrand_rarity': 122464, 'iSlingShlong': 118595, 'hilltopye': 81245, 'tefunka': 79560, 'OldFashionedJizz': 64398, 'JLBesq1981': 58235, 'rspix000': 57107, 'Wagamaga': 47989, 'stem12345679': 47455, 'TheJeck': 26058, 'TheGamerDanYT': 25357, 'TrumpSharted': 21154, 'NotsoPG': 18518, 'SonictheManhog': 18116, 'BlanketMage': 13677, 'NewAltWhoThis': 12771, 'kevinmrr': 11900, 'Dajakesta0624': 11613, 'apocalypticalley': 10382}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woOFrPFQT5cZ"
      },
      "source": [
        "### P1.2.2 - Awarded posts\n",
        "\n",
        "Find the number of posts that have received at least one award. Your query should return only one value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fVuaWmmUGVW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f02aa33-0afa-44bd-a963-b4394d3e3f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of posts that have received at least *One Award* = 119\n"
          ]
        }
      ],
      "source": [
        "#printing query with atleast one award.\n",
        "print(\"The number of posts that have received at least *One Award* =\", len(df.query('total_awards_received > 0')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVj1WikSUPjO"
      },
      "source": [
        "### P1.2.3 Find Covid \n",
        "\n",
        "Find the name and description of all subreddits where the name starts with `Covid` or `Corona` and the description contains `covid` or `Covid` anywhere. Your code should generate a dictionary of the form#\n",
        "\n",
        "```python\n",
        "  {'Coronavirus':'Place to discuss all things COVID-related',\n",
        "  ...\n",
        "  }\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6fIWO8BUhu3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7214b543-f60c-4a7e-8078-d978382a5248"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'COVID': 'COVID-19 News, Etc.',\n",
              " 'COVID19': 'In December 2019, SARS-CoV-2, the virus causing the disease COVID-19, emerged in the city of Wuhan, China. This subreddit seeks to facilitate scientific discussion of this global public health threat.',\n",
              " 'Coronavirus': 'Place to discuss all things COVID-related',\n",
              " 'CoronavirusCA': 'Tracking the Coronavirus/Covid-19 outbreak in California',\n",
              " 'CoronavirusDownunder': 'This subreddit is a place to share news, information, resources, and support that relate to the novel coronavirus SARS-CoV-2 and the disease it causes called COVID-19. The primary focus of this sub is to actively monitor the situation in Australia, but all posts on international news and other virus-related topics are welcome, to the extent they are beneficial in keeping those in Australia informed.',\n",
              " 'CoronavirusUS': 'USA/Canada specific information on the coronavirus (SARS-CoV-2) that causes coronavirus disease 2019 (COVID-19)'}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#creating new data and converting it into dictionary.\n",
        "convert_dict =dict(df[['subreddit', 'subr_description']].values)\n",
        "\n",
        "#checking dictionary.\n",
        "convert_dict\n",
        "\n",
        "#filtering all the key words that starts with corona and covid and description contains coivd. \n",
        "covid = {key: convert_dict[key] for key in convert_dict.keys()\n",
        " & {'Coronavirus','COVID','COVID19','CoronavirusCA','CoronavirusDownunder','CoronavirusUS'}}\n",
        "\n",
        "#output dictionary.\n",
        "covid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToPttp2-fsXG"
      },
      "source": [
        "### P1.2.4 - Redditors that favorite the most\n",
        "\n",
        "Find the users that have favorited the largest number of subreddits. You must produce a pandas dataframe with **two** columns, with the following format:\n",
        "\n",
        "```python\n",
        "     redditor\t    numb_favs\n",
        "0\tuser1           7\n",
        "1\tuser2           6\n",
        "2\tuser3\t       5\n",
        "3\tuser4           4\n",
        "...\n",
        "```\n",
        "\n",
        "where the first column is a Redditor username and the second column is the number of distinct subreddits he/she has favorited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbFeie3jip44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "8262bd64-462a-4135-dca9-ace946703a61"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  redditor  numb_favs\n",
              "0           magnusthered15          7\n",
              "1                 ry_ta506          6\n",
              "2     FriendlyVegetable420          6\n",
              "3              Flippy-Fish          6\n",
              "4                KarmaFury          6\n",
              "...                    ...        ...\n",
              "1593              XVI_ONYX          1\n",
              "1594          kimcheefarts          1\n",
              "1595           Real_Quarit          1\n",
              "1596     hildebrand_rarity          1\n",
              "1597        lilstinky[***]          1\n",
              "\n",
              "[1598 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-16797c12-6e30-4da1-bf4d-ac94b8541583\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>redditor</th>\n",
              "      <th>numb_favs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>magnusthered15</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ry_ta506</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>FriendlyVegetable420</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Flippy-Fish</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KarmaFury</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1593</th>\n",
              "      <td>XVI_ONYX</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>kimcheefarts</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>Real_Quarit</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>hildebrand_rarity</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>lilstinky[***]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1598 rows  2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-16797c12-6e30-4da1-bf4d-ac94b8541583')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-16797c12-6e30-4da1-bf4d-ac94b8541583 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-16797c12-6e30-4da1-bf4d-ac94b8541583');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#creating new data by droping all duplicate values from subr_faved_by_as_list and subreddit.\n",
        "a = df[['subr_faved_by_as_list','subreddit']].drop_duplicates(subset=['subreddit'])['subr_faved_by_as_list']\n",
        "\n",
        "#counting all values from a.\n",
        "b = pd.Series([x for y in a for x in y]).value_counts()\n",
        "\n",
        "#converting all b values into dictionary.\n",
        "c = dict(b)\n",
        "\n",
        "#creating new dataframe for redditor and numb_favs.\n",
        "Favorite_users = pd.DataFrame(columns=[\"redditor\",\"numb_favs\"])\n",
        "\n",
        "#adding keys from dictionary to redditor column.\n",
        "Favorite_users[\"redditor\"] = c.keys()\n",
        "\n",
        "#adding values from dictionary to numb_favs column.\n",
        "Favorite_users[\"numb_favs\"] = c.values()\n",
        "\n",
        "#final output.\n",
        "Favorite_users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsAF9jpblJLp"
      },
      "source": [
        "## P1.3 Ethics \n",
        "\n",
        "Imagine you are **the head of a data mining company that needs to use** the insights gained in this assignment to scan social media for covid-related content, and automatically flag it as conspiracy or not conspiracy (for example, for hiding potentially harmful tweets or Facebook posts). **Some information about the project and the team:**\n",
        "\n",
        " - Your client is a political party concerned about misinformation.\n",
        " - The project requires mining Facebook, Reddit and Instagram data.\n",
        " - The team consists of Joe, an American mathematician who just finished college; Fei, a senior software engineer from China; and Francisco, a data scientist from Spain.\n",
        "\n",
        "Reflect on the impact of exploiting data science for such an application. You should map your discussion to one of the five actions outlined in the UKs Data Ethics Framework. \n",
        "\n",
        "Your answer should address the following:\n",
        "\n",
        " - Identify the action **in which your project is the weakest**.\n",
        " - Then, justify your choice by critically analyzing the three key principles **for that action** outlined in the Framework, namely transparency, accountability and fairness.\n",
        " - Finally, you should propose one solution that explicitly addresses one point related to one of these three principles, reflecting on how your solution would improve the data cycle in this particular use case.\n",
        "\n",
        "Your answer should be between 500 and 700 words. **You are strongly encouraged to follow a scholarly approach, e.g., with references to peer reviewed publications. References do not count towards the word limit.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YJQSO8Amuea"
      },
      "source": [
        "## Answer -  \n",
        "**Data Ethics Framework:**\n",
        "\n",
        "The Data Ethics Framework is a collection of guidelines outlined by the government to regulate the proper use of data in the public sector. This guideline is designed for anybody who works directly or indirectly with data in the public sector, including data practitioners such as data analysts, data scientists, statisticians, and those who contribute to the production of data insights. Throughout the process of planning, developing, and evaluating a new project, teams should go through the framework jointly. Each component of the framework is intended to be examined frequently during the project, particularly when data collecting, storage, analysis, or sharing methods change (Data Ethics Framework, 2020).\n",
        "\n",
        "**Project Introduction:**\n",
        "\n",
        "In this project, the company needs to mine data from social media for covid-related insights and automatically classify them as conspiracy or non-conspiracy. This project has three team members: one American Mathematician, one Chinese software engineer, and one Spanish data scientist. The company's customer is a political party, and the project involves the mining of data from Facebook, Reddit, and Instagram.\n",
        "Weakness:\n",
        "There are some flaws in this project. The first is that American Mathematician Joe recently graduated from college, thus he lacks expertise in his profession. However, the primary flaw of this project is that all team members are not UK residents. As a result, they lack a sufficient grasp of UK Data Ethics and its Framework. This will cause issues while complying with the law.\n",
        "\n",
        "**Key Principles:**\n",
        "\n",
        "Complying with the law is one of the specified acts in the UK Data Ethics Framework. Everyone in the team must be familiar with the applicable laws and rules of practice governing data usage (Data Ethics Framework, 2020). When in doubt, seek the advice of relevant experts. Always take into account all of the questions from the ethics framework. This questionnaire is available for download from the UK Government's website. For each specific action, there are three fundamental key principles that can benefit all phases of the project. These principles are as follows:\n",
        "\n",
        "*Transparency* - In terms of transparency, team members are responsible for three main tasks: 1) Justify Process, 2) Clarify Content and Explain Outcome, and 3) Justify Outcome (Leslie, 2019). It is important to establish the project's trustworthiness via openness to public inspection and transparency of processes throughout the project's lifespan. It will guarantee that decisions are clear and will assist the research's credibility (ReedBerendt, Dove, and Pareek, 2021). Projects can benefit the most from an engaged, transparent, and reflective approach.\n",
        "\n",
        "*Accountability* - At a high level, the organization and information assurance teams will be in charge of this, including ensuring policies and standards are in place. However, it is critical to demonstrate how everyone doing on this on an individual level, such as by detailed documentation of topics such as DPIA (Data Protection Impact Assessments) (Data Ethics Framework, 2020). This helps to track all activities of the project legally.\n",
        "\n",
        "*Fairness* - The third concept is fairness, which is essential for removing any discriminatory impacts on people and social groups, even if this prejudice is inadvertent (Schwab, 2021). If project outcomes are produced using biased, corrupted, or distorted datasets, affected stakeholders will not be appropriately protected against discriminating damage.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "After examining all three key principles, transparency would be the best answer for this corporate project since it would be open to inspection and free of secrets.\n",
        "\n",
        "**References:**\n",
        "\n",
        "Assets.publishing.service.gov.uk. 2020. Data Ethics Framework. [online] Available at: <https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/923108/Data_Ethics_Framework_2020.pdf> [Accessed 26 April 2022].\n",
        "\n",
        "Leslie, D., 2019. Understanding Artificial Intelligence Ethics and Safety: A Guide for the Responsible Design and Implementation of AI Systems in the Public Sector. SSRN Electronic Journal, [online] Available at: <https://www.turing.ac.uk/sites/default/files/2019-06/understanding_artificial_intelligence_ethics_and_safety.pdf> [Accessed 26 April 2022].\n",
        "\n",
        "ReedBerendt, R., Dove, E. and Pareek, M., 2021. The Ethical Implications of Big Data Research in Public Health: Big Data Ethics by Design in the UKREACH Study. Ethics &amp; Human Research, 44(1), pp.2-17. [Accessed 26 April 2022].\n",
        "\n",
        "Schwab, P., 2021. The UK Data Ethics Framework Explained. [online] Intotheminds. Available at: <https://www.intotheminds.com/blog/en/uk-data-ethics-framework-explained/> [Accessed 26 April 2022].\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}